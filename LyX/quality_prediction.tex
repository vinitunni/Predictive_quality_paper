%% LyX 2.0.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[twoside,english]{elsarticle}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\pagestyle{headings}
\usepackage{verbatim}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% specify here the journal
\journal{Example: Nuclear Physics B}

% use this if you need line numbers
%\usepackage{lineno}

\makeatother

\usepackage{babel}
\begin{document}





\title{Predictive Quality Control\tnoteref{t1,t2}}


\tnotetext[t1]{This document is a collaborative effort.}


\author[rvt]{C.V.~Radhakrishnan\fnref{fn1}\corref{cor1}\corref{cor2}}


\ead{cvr@river-valley.com}


\author[rvt,focal]{K.~Bazargan\fnref{fn2}}


\ead[url]{http://www.elsevier.com}


\fntext[fn1]{This is the specimen author footnote.}


\fntext[fn2]{Another author footnote, but a little more longer.}


\cortext[cor1]{Corresponding author}


\cortext[cor2]{Principal corresponding author}


\address[rvt]{River Valley Technologies, SJP Building, Cotton Hills, Trivandrum,
Kerala, India 695014}


\address[focal]{River Valley Technologies, 9, Browns Court, Kennford, Exeter, United
Kingdom}
\begin{abstract}
Abstract, should normally be not longer than 200 words.\end{abstract}
\begin{keyword}
quadruple exiton \sep polariton \sep WGM \PACS 71.35.-y \sep 71.35.Lk
\sep 71.36.+c \MSC[2008]23-557
\end{keyword}
\maketitle



\section{Introduction}


\subsection{How is quality measured in present time?}

Present day quality measurements are done using various metrological
tools which quantify various shapes and/or form features of the part.
These features are given their individual limits by the designer which
help a metrologist in determining whether a part that has just been
manufactured is fit to be deemed a good part or not. However, most
of these tools are completely external equipments which need to be
explicitly used over the part(s) to see the results. This usually
calls for extra man-power and reduced throughput. So what manufacturers
end up doing is that they keep an inspection frequency for non critical
parts where quality checks on these parts are made at regular intervals
until one sees an anomaly. How this anomaly is responded to again
depends on what feature we are measuring.

The main problem with such a way of measurement is that :-
\begin{enumerate}
\item We have to make a compromise between the throughput and inspection
interval.
\item As the quality measurement is isolated from part manufacturing, you
can\textquoteright{}t detect a bad part unless it has already been
manufactured.
\end{enumerate}

\subsection{How do we propose to measure it?}

The main gist of the ideology we have applied is that, when a part
is being made, the information of each cut made by the machine is
reflected in some real-time aspect of the machine. It could be noise,
power, vibration, loads, etc. So what we do is, we record these variables
(in our case, power) real-time using appropriate tools and find a
correlation between these readings and the quality of the part manufactured.
This correlation can be tightened depending on the kind of accuracy
one is looking for. It also has an advantage over the conventional
approach in certain key aspects such as:-
\begin{enumerate}
\item The computational time is significantly less once a correlation model
has been made. Also, buying computational time is cheaper than wasting
producing time.
\item The feedback we get is near to real time and we could infact get early
warnings as the part is being made.
\item The kind of investment needed for this is much lesser compared to
buying precise instruments which, other than being pretty expensive,
usually have only a single functionality.
\end{enumerate}

\section{Methodology}


\subsection{Getting Quality data}

As we need to correlate between power readings during a producing
cycle and quality, we are essentially trying to create a model or
create a procedure to generate a model which would take the power
readings as input and give the quality as output. Thus, we needed
some input quality data for a training set.

For this, we monitored the Ra values of a part, which underwent turning
operation, at regular intervals (once in 25 parts for first 500 cycles
and once in every 10 parts from there on) for 1030 parts. These measurements
were made at two different faces and three different locations in
each face. We observed a downward trend in the Ra values for the first
300 parts after which they were more or less constant (no particular
trend) till part 3000 after which they again rose up and went outside
the specification limit.

So we decided to use the trend evident in the first 300 parts for
our model creation and verification. We created a 2 degree polynomial
fit for the first three hundred parts using 12 readings as out control
points.%
\begin{comment}
INSERT FIGURE AND POLYFIT STATS.
\end{comment}


Now this sample of 300 points was used as our quality data sample
which would be used for training and verification.


\subsection{Detect Cycles}

For input of the model we decided to use power readings. The reason
for this is BLEEAARRGGH\{%
\begin{comment}
INSERT REASON FOR SELECTING POWER
\end{comment}
\}. As we plan to make here an autonomous correlation model, we need
to be able to correctly identify the start and end times of a production
cycle. In relatively newer machines, this data can be obtained via
the controller. However, the number of machines which are not capable
of that are quite large. So we need to come up with a method to let
the machine identify a cycle and record its start and end time. We
use a simple pattern matching approach here. We are going to analyze
the power readings we get as a stream of data and in it look for recognisable
patterns which would indicate whether and where a cycle occurred.
So to start, we need a known producing cycle which we can use as a
template for comparisons. Once we decide this template we use an algorithm
called \textbf{Dynamic Time Warping} (DTW).


\subsubsection{Dynamic Time warping}

BLLEEEAAAARRRRGGGHHH%
\begin{comment}
INSERT EXPLANATION AND FIGURES HERE.
\end{comment}



\subsection{Extracting input from power consumption pattern}

Now we have the start and end times for 300 producing cycles and we
can use this data to get power consumption patterns for each individual
part and use it as necessary. However, we can't use these patterns
directly as inputs for any machine llearning algorithm as it has many
inherent problems. The main reason being that small time offsets in
the time series data could throw off the result by a large amount.
So we basically needed to process our power pattern to extract more
stable information from it. However in the process of making it stable,
we also have to make sure that we don\textquoteright{}t lose the individuality
of the pattern as we depend on this individual character for our quality
reading. We resolved this using two methods:-
\begin{enumerate}
\item Scalar extractions from pattern.
\item Cycle splitting.
\end{enumerate}

\subsubsection{Scalar extractions from splitting}

Here we extract from our cycle a set of scalars which are stable and
do not get affected due to small changes like minor time scaling,
peaks,etc. These scalars could be mean, standard deviation, DTW distance,
median, mode, amplitude, etc. However, as pointed out earlier, using
these scalars alone would render many cycles to be same as we are
wasting a lot of information. So we use, not one, but a combination
of these scalars as now we get more options of variability thus giving
us our desired result of stable and unique inputs.


\subsubsection{Cycle splitting}

This is a much more important process and increases the effectiveness
of scalar extraction. What we essentially do cut the cycle into a
number of parts using different methods (which will be discussed later)
and apply the scalar extraction for each segment. So now we increase
our number of inputs and also increase the individuality of each input
in a stable way. 

Much more importantly, when we later analyze our results, we can actually
pinpoint which segments of the cycle are primarily responsible for
the quality parameter being measured (in our case, Ra). This kind
of information can prove to be really useful as now we can know what
part of the cycle is primarily responsible for which quality parameter.
This can in turn help us detecting tool damages when we observe a
deviation in certain quality parameter. Splitting the cycle gives
us one more important advantage. Now we can calculate the quality
metric of a cycle by including one split at a time, into our input
set, sequentially. This can be really helpful in predicting parameter
qualities before the machining is over and thus can help us in taking
corrective action.

We primarily employed four different methods to split cycle. They
were:-
\begin{enumerate}
\item \textbf{Uniform splitting}: As the name suggests, we split the cycle
into equally sized sections. The number of parts would depend on how
much accuracy we want and how much computation cost we can afford. 
\item \textbf{Forward feedback splitting}: This is a process in which we
make a cut by splitting a section into two equal halves and decide
whether we keep that cut by checking if the amount of improvement
we get is significant enough. It is slightly biased to create more
cuts in the beginning of the cycle as we move on to the former section
after a cut has been made and move to the next section only if no
more cuts can be made in the former one. 
\item \textbf{Reverse feedback splitting}: This is almost like the Forward
Feedback Splitting but instead of biasing cuts to the start, this
one biases cuts to the end. 
\item \textbf{Phase dependent cuts}: These are cuts which signify a change
in process in the machining. Theoretically, these should be the most
optimum cuts as they point towards specific process. However, finding
them out autonomously can be tricky if wanted for all situations thus
these cuts have to be entered manually.
\end{enumerate}

\subsection{Model Selection}

There are a plethora%
\begin{comment}
CHECK IF THERE ACTUALLY ARE A PLETHORA OF METHODS
\end{comment}
{} of machine learning alg

\appendix

\section{Appendix name}

Appendix, only when needed.


\section*{-----------------}

You can use either Bib\TeX{}:

\bibliographystyle{elsarticle-harv}
\addcontentsline{toc}{section}{\refname}\bibliography{../examples/biblioExample}



\section*{---------------------}

\noindent Or plain bibliography:
\begin{thebibliography}{References}
\bibitem{key-1}Frank Mittelbach and Michel Goossens: \emph{The \LaTeX{}
Companion Second Edition.} Addison-Wesley, 2004.

\bibitem{key-2}Scott Pakin. The comprehensive \LaTeX{} symbol list,
2005.\end{thebibliography}

\end{document}
